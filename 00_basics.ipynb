{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m518.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in ./.venv/lib/python3.10/site-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (63.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in ./.venv/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# download large spacy model\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: EMAIL_ADDRESS, start: 75, end: 97, score: 1.0\n",
      "type: PERSON, start: 16, end: 21, score: 0.85\n",
      "type: PHONE_NUMBER, start: 46, end: 58, score: 0.75\n",
      "type: URL, start: 86, end: 97, score: 0.5\n"
     ]
    }
   ],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "\n",
    "text = \"His name is Mr. Jones and his phone number is 212-555-5555 and my email is stevejones@hotmail.com. My address is 12 Pine Road, KT6 8LP\"\n",
    "\n",
    "analyzer = AnalyzerEngine()\n",
    "analyzer_results = analyzer.analyze(text=text, language=\"en\")\n",
    "\n",
    "for r in analyzer_results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch data speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average chars: 172.3057\n",
      "Average words: 24.9396\n",
      "Total Words: 249396\n",
      "sample_text: A student's assessment was found on device bearing IMEI: 06-184755-866851-3. The document falls under the various topics discussed in our Optimization curriculum. Can you please collect it?\n"
     ]
    }
   ],
   "source": [
    "# batch analyze:\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"ai4privacy/pii-masking-200k\",split='train')\n",
    "examples = [data[i]['source_text'] for i in range(10000)]\n",
    "char_count = [len(text) for text in examples]\n",
    "word_count = [len(text.split(' ')) for text in examples]\n",
    "import numpy as np\n",
    "print('Average chars:', np.mean(char_count))\n",
    "print('Average words:', np.mean(word_count))\n",
    "print('Total Words:', np.sum(word_count))\n",
    "print('sample_text:', examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse time: 30.002370357513428\n",
      "Redact time: 0.0771169662475586\n"
     ]
    }
   ],
   "source": [
    "from presidio_analyzer import BatchAnalyzerEngine\n",
    "from presidio_anonymizer import BatchAnonymizerEngine\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "batch_analyzer = BatchAnalyzerEngine(analyzer_engine=analyzer)\n",
    "analyzer_results = batch_analyzer.analyze_dict({'examples':examples}, language=\"en\", entities = ['PHONE_NUMBER', 'EMAIL_ADDRESS', 'ID', 'CREDIT_CARD', 'LOCATION'])\n",
    "analyzer_results = list(analyzer_results)\n",
    "print('Analyse time:', time.time()-t0)\n",
    "\n",
    "t0 = time.time()\n",
    "batch_anonymizer = BatchAnonymizerEngine()\n",
    "anonymizer_results = batch_anonymizer.anonymize_dict(analyzer_results)\n",
    "anonymizer_results = list(anonymizer_results)\n",
    "print('Redact time:', time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING ALL DETECTORS (sometimes gives overflow error so we resort back to list comprehension)\n",
    "def analyse_text(text):\n",
    "    try:\n",
    "        return analyzer.analyze(text, language=\"en\")\n",
    "    except OverflowError:\n",
    "        return 'OVERFLOW'\n",
    "results = [analyse_text(t) for t in examples[:2000]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deny List Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Professor as TITLE\n"
     ]
    }
   ],
   "source": [
    "from presidio_analyzer import PatternRecognizer\n",
    "# look for titles and titles only\n",
    "titles_list = [\"Sir\",\"Ma'am\",\"Madam\",\"Mr.\",\"Mrs.\",\"Ms.\",\"Miss\",\"Dr.\",\"Professor\",]\n",
    "titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\", deny_list=titles_list)\n",
    "text1 = \"I suspect Professor Plum, in the Dining Room, with the candlestick. His email is profplum@gmail.com\"\n",
    "results = titles_recognizer.analyze(text1, entities=[\"TITLE\"])\n",
    "for result in results:\n",
    "    print(f\"- {text1[result.start:result.end]} as {result.entity_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Professor as TITLE\n",
      "- profplum@gmail.com as EMAIL_ADDRESS\n"
     ]
    }
   ],
   "source": [
    "# add it to a the analyzer\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "\n",
    "analyzer = AnalyzerEngine()\n",
    "analyzer.registry.add_recognizer(titles_recognizer)\n",
    "results = analyzer.analyze(text1, entities=[\"TITLE\", \"EMAIL_ADDRESS\"], language=\"en\")\n",
    "for result in results:\n",
    "    print(f\"- {text1[result.start:result.end]} as {result.entity_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGEX Recogniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "[type: NUMBER, start: 10, end: 13, score: 0.5]\n"
     ]
    }
   ],
   "source": [
    "from presidio_analyzer import Pattern, PatternRecognizer\n",
    "\n",
    "# Define the regex pattern in a Presidio `Pattern` object:\n",
    "numbers_pattern = Pattern(name=\"numbers_pattern\", regex=\"\\d+\", score=0.5)\n",
    "\n",
    "# Define the recognizer with one or more patterns\n",
    "number_recognizer = PatternRecognizer(\n",
    "    supported_entity=\"NUMBER\", patterns=[numbers_pattern])\n",
    "\n",
    "text2 = \"I live in 510 Broad st.\"\n",
    "\n",
    "numbers_result = number_recognizer.analyze(text=text2, entities=[\"NUMBER\"])\n",
    "\n",
    "print(\"Result:\")\n",
    "print(numbers_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: NUMBER, start: 10, end: 13, score: 0.5\n",
      "type: NUMBER, start: 41, end: 42, score: 0.5\n",
      "type: NUMBER, start: 43, end: 44, score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# add it to a the analyzer\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "\n",
    "text = \"I live in 510 Broad st. My postcode is KT7 8LP\"\n",
    "analyzer = AnalyzerEngine()\n",
    "analyzer.registry.add_recognizer(number_recognizer)\n",
    "results = analyzer.analyze(text, entities=[\"NUMBER\"], language=\"en\")\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Based Recognizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "type: PERSON, start: 0, end: 7, score: 0.85 : Roberto\n",
      "type: NUMBER, start: 17, end: 21, score: 0.7 : Five\n",
      "type: NUMBER, start: 22, end: 24, score: 0.7 : 10\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from presidio_analyzer import EntityRecognizer, RecognizerResult\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
    "\n",
    "\n",
    "class NumbersRecognizer(EntityRecognizer):\n",
    "\n",
    "    \"\"\"\n",
    "    load an analyse methods are required\n",
    "    we use spacy's token.like_number method to check if a token is a number or not\n",
    "    \"\"\"\n",
    "\n",
    "    expected_confidence_level = 0.7  # expected confidence level for this recognizer\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \"\"\"No loading is required.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def analyze(\n",
    "        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts\n",
    "    ) -> List[RecognizerResult]:\n",
    "        \"\"\"\n",
    "        Analyzes test to find tokens which represent numbers (either 123 or One Two Three).\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # iterate over the spaCy tokens, and call `token.like_num`\n",
    "        for token in nlp_artifacts.tokens:\n",
    "            if token.like_num:\n",
    "                result = RecognizerResult(\n",
    "                    entity_type=\"NUMBER\",\n",
    "                    start=token.idx,\n",
    "                    end=token.idx + len(token),\n",
    "                    score=self.expected_confidence_level,\n",
    "                )\n",
    "                results.append(result)\n",
    "        return results\n",
    "\n",
    "\n",
    "# Instantiate the new NumbersRecognizer:\n",
    "new_numbers_recognizer = NumbersRecognizer(supported_entities=[\"NUMBER\"])\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "\n",
    "text = \"Roberto lives in Five 10 Broad st. KT7 8HG\"\n",
    "analyzer = AnalyzerEngine()\n",
    "analyzer.registry.add_recognizer(new_numbers_recognizer)\n",
    "\n",
    "results = analyzer.analyze(text=text, language=\"en\")\n",
    "print(\"Results:\")\n",
    "for result in results:\n",
    "    print(str(result) + \" : \" + text[result.start:result.end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levaraging Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      " [type: UK_POST_CODE, start: 39, end: 46, score: 0.95]\n"
     ]
    }
   ],
   "source": [
    "# combine a regex search with context words\n",
    "\n",
    "from presidio_analyzer import Pattern, PatternRecognizer, RecognizerRegistry, AnalyzerEngine\n",
    "from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n",
    "\n",
    "\n",
    "# Define the regex pattern\n",
    "regex = r\"[A-Z]{1,2}\\d[A-Z\\d]? ?\\d[A-Z]{2}\"  # uk post code\n",
    "\n",
    "postcode_pattern = Pattern(name=\"uk post code\", regex=regex, score=0.5)\n",
    "\n",
    "# Define the recognizer with the defined pattern\n",
    "postcode_recognizer = PatternRecognizer(\n",
    "    supported_entity=\"UK_POST_CODE\", patterns=[postcode_pattern], context=['post', 'code', 'address'], # context increases by 0.4\n",
    ")\n",
    "\n",
    "context_aware_enhancer = LemmaContextAwareEnhancer(\n",
    "    context_similarity_factor=0.45, # added to score if context words present\n",
    "    min_score_with_context_similarity=0.4 # only flag if above this threshold\n",
    ")\n",
    "\n",
    "registry = RecognizerRegistry()\n",
    "registry.add_recognizer(postcode_recognizer)\n",
    "analyzer = AnalyzerEngine(registry=registry, context_aware_enhancer=context_aware_enhancer)\n",
    "\n",
    "# Test\n",
    "text = \"I live in 510 Broad st. My postcode is KT7 8LP. \"\n",
    "results = analyzer.analyze(text=text, language=\"en\")\n",
    "\n",
    "print(f\"Result:\\n {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "[type: UK_POST_CODE, start: 0, end: 7, score: 0.85]\n"
     ]
    }
   ],
   "source": [
    "# context can be added at analyse time like so:\n",
    "# e.g. we have a column name present:\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, PatternRecognizer\n",
    "\n",
    "# Define the recognizer with the defined pattern and context words\n",
    "postcode_recognizer = PatternRecognizer(\n",
    "    supported_entity=\"UK_POST_CODE\",\n",
    "    patterns=[postcode_pattern],\n",
    "    context=[\"address\", \"post\"],\n",
    ")\n",
    "registry = RecognizerRegistry()\n",
    "registry.add_recognizer(postcode_recognizer)\n",
    "analyzer = AnalyzerEngine(registry=registry)\n",
    "\n",
    "# Test with an example record having a column name which could be injected as context\n",
    "record = {\"column_name\": \"address\", \"text\": \"HY8 9LH\"}\n",
    "\n",
    "result = analyzer.analyze(\n",
    "    text=record[\"text\"], language=\"en\", context=[record[\"column_name\"]]\n",
    ")\n",
    "\n",
    "print(\"Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
